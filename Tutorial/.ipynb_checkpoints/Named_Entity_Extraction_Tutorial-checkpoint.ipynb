{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Extraction Tutorial\n",
    "This tutorial is a slight modification of the tutorial by Sam Galen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn version: 0.19.2\n",
      "Libraries succesfully loaded!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import io\n",
    "import nltk\n",
    "import scipy\n",
    "import codecs\n",
    "import sklearn\n",
    "import pycrfsuite\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('sklearn version:', sklearn.__version__)\n",
    "print('Libraries succesfully loaded!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2features(sent, feature_func):\n",
    "    return [feature_func(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    #print('sent', sent)\n",
    "    return [s[-1] for s in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [s[0] for s in sent]\n",
    "\n",
    "def bio_classification_report(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Classification report for a list of BIO-encoded sequences.\n",
    "    It computes token-level metrics and discards \"O\" labels.\n",
    "    \n",
    "    Note that it requires scikit-learn 0.15+ (or a version from github master)\n",
    "    to calculate averages properly!\n",
    "    \"\"\"\n",
    "    lb = LabelBinarizer()\n",
    "    y_true_combined = lb.fit_transform(y_true)\n",
    "    y_pred_combined = lb.transform(y_pred)\n",
    "        \n",
    "    tagset = set(lb.classes_) - {'O'}\n",
    "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "    \n",
    "    return classification_report(\n",
    "        y_true_combined,\n",
    "        y_pred_combined,\n",
    "        labels = [class_indices[cls] for cls in tagset],\n",
    "        target_names = tagset,\n",
    "    )\n",
    "\n",
    "def generate_kaggle_res_file(ids, labels, file_path):\n",
    "    \"\"\"\n",
    "    Generate result file for submitting to Kaggle.\n",
    "    ids - the id for the tokens in test file\n",
    "          should be in the same order as test file\n",
    "    labels - the predictted label for each token\n",
    "    file_path - the path includes the filename where\n",
    "                you want to save the result\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w') as res_file:\n",
    "        res_file.write('id,label\\n')\n",
    "        for i,l in zip(ids, labels):\n",
    "            res_file.write('{},{}\\n'.format(i,l))\n",
    "            \n",
    "def word2simple_features(sent, i):\n",
    "    '''\n",
    "    This makes a simple baseline.  \n",
    "    You can add and/or remove features to get (much?) better results.\n",
    "    Experiment with it as you will need to do this for assignment.\n",
    "    '''\n",
    "    word = sent[i][0]\n",
    "    \n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-2:]': word[-2:],\n",
    "    }\n",
    "    if i == 0:\n",
    "        features['BOS'] = True\n",
    "        \n",
    "    if i == len(sent)-1:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "# load data and preprocess\n",
    "def extract_data(path):\n",
    "    \"\"\"\n",
    "    Extracting data from train file or test file. \n",
    "    path - the path of the file to extract\n",
    "    \n",
    "    return:\n",
    "        res - a list of sentences, each sentence is a\n",
    "              a list of tuples. For train file, each tuple\n",
    "              contains token and label. For test file, each\n",
    "              tuple only contains token.\n",
    "        ids - a list of ids for the corresponding token. This\n",
    "              is mainly for Kaggle submission.\n",
    "    \"\"\"\n",
    "    #with open(path) as file:\n",
    "    file = io.open(path, mode=\"r\", encoding=\"utf-8\")\n",
    "    next(file)\n",
    "    res = []\n",
    "    ids = []\n",
    "    sent = []\n",
    "    for line in file:\n",
    "        if line != '\\n':\n",
    "            parts = line.strip().split(' ')\n",
    "            sent.append(tuple(parts[1:]))\n",
    "            ids.append(parts[0])\n",
    "        else:\n",
    "            res.append(sent)\n",
    "            sent = []\n",
    "                \n",
    "    return res, ids\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a NER classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and Test data upload succesfully!\n",
      "Feature Extraction done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'BOS': True,\n",
       "  'bias': 1.0,\n",
       "  'word.lower()': u'tambi\\xe9n',\n",
       "  'word[-2:]': u'\\xe9n'},\n",
       " {'bias': 1.0, 'word.lower()': u'el', 'word[-2:]': u'el'},\n",
       " {'bias': 1.0, 'word.lower()': u'secretario', 'word[-2:]': u'io'},\n",
       " {'bias': 1.0, 'word.lower()': u'general', 'word[-2:]': u'al'},\n",
       " {'bias': 1.0, 'word.lower()': u'de', 'word[-2:]': u'de'},\n",
       " {'bias': 1.0, 'word.lower()': u'la', 'word[-2:]': u'la'},\n",
       " {'bias': 1.0, 'word.lower()': u'asociaci\\xf3n', 'word[-2:]': u'\\xf3n'},\n",
       " {'bias': 1.0, 'word.lower()': u'espa\\xf1ola', 'word[-2:]': u'la'},\n",
       " {'bias': 1.0, 'word.lower()': u'de', 'word[-2:]': u'de'},\n",
       " {'bias': 1.0, 'word.lower()': u'operadores', 'word[-2:]': u'es'},\n",
       " {'bias': 1.0, 'word.lower()': u'de', 'word[-2:]': u'de'},\n",
       " {'bias': 1.0, 'word.lower()': u'productos', 'word[-2:]': u'os'},\n",
       " {'bias': 1.0, 'word.lower()': u'petrol\\xedferos', 'word[-2:]': u'os'},\n",
       " {'bias': 1.0, 'word.lower()': u',', 'word[-2:]': u','},\n",
       " {'bias': 1.0, 'word.lower()': u'aurelio', 'word[-2:]': u'io'},\n",
       " {'bias': 1.0, 'word.lower()': u'ayala', 'word[-2:]': u'la'},\n",
       " {'bias': 1.0, 'word.lower()': u',', 'word[-2:]': u','},\n",
       " {'bias': 1.0, 'word.lower()': u'ha', 'word[-2:]': u'ha'},\n",
       " {'bias': 1.0, 'word.lower()': u'negado', 'word[-2:]': u'do'},\n",
       " {'bias': 1.0, 'word.lower()': u'la', 'word[-2:]': u'la'},\n",
       " {'bias': 1.0, 'word.lower()': u'existencia', 'word[-2:]': u'ia'},\n",
       " {'bias': 1.0, 'word.lower()': u'de', 'word[-2:]': u'de'},\n",
       " {'bias': 1.0, 'word.lower()': u'cualquier', 'word[-2:]': u'er'},\n",
       " {'bias': 1.0, 'word.lower()': u'tipo', 'word[-2:]': u'po'},\n",
       " {'bias': 1.0, 'word.lower()': u'de', 'word[-2:]': u'de'},\n",
       " {'bias': 1.0, 'word.lower()': u'acuerdos', 'word[-2:]': u'os'},\n",
       " {'bias': 1.0, 'word.lower()': u'sobre', 'word[-2:]': u're'},\n",
       " {'bias': 1.0, 'word.lower()': u'los', 'word[-2:]': u'os'},\n",
       " {'bias': 1.0, 'word.lower()': u'precios', 'word[-2:]': u'os'},\n",
       " {'bias': 1.0, 'word.lower()': u',', 'word[-2:]': u','},\n",
       " {'bias': 1.0, 'word.lower()': u'afirmando', 'word[-2:]': u'do'},\n",
       " {'bias': 1.0, 'word.lower()': u'que', 'word[-2:]': u'ue'},\n",
       " {'bias': 1.0, 'word.lower()': u'\\xfanicamente', 'word[-2:]': u'te'},\n",
       " {'bias': 1.0, 'word.lower()': u'es', 'word[-2:]': u'es'},\n",
       " {'bias': 1.0, 'word.lower()': u'la', 'word[-2:]': u'la'},\n",
       " {'bias': 1.0, 'word.lower()': u'cotizaci\\xf3n', 'word[-2:]': u'\\xf3n'},\n",
       " {'bias': 1.0, 'word.lower()': u'internacional', 'word[-2:]': u'al'},\n",
       " {'bias': 1.0, 'word.lower()': u'la', 'word[-2:]': u'la'},\n",
       " {'bias': 1.0, 'word.lower()': u'que', 'word[-2:]': u'ue'},\n",
       " {'bias': 1.0, 'word.lower()': u'pone', 'word[-2:]': u'ne'},\n",
       " {'bias': 1.0, 'word.lower()': u'de', 'word[-2:]': u'de'},\n",
       " {'bias': 1.0, 'word.lower()': u'acuerdo', 'word[-2:]': u'do'},\n",
       " {'bias': 1.0, 'word.lower()': u'a', 'word[-2:]': u'a'},\n",
       " {'bias': 1.0, 'word.lower()': u'todos', 'word[-2:]': u'os'},\n",
       " {'bias': 1.0, 'word.lower()': u'los', 'word[-2:]': u'os'},\n",
       " {'bias': 1.0, 'word.lower()': u'pa\\xedses', 'word[-2:]': u'es'},\n",
       " {'EOS': True, 'bias': 1.0, 'word.lower()': u'.', 'word[-2:]': u'.'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load train and test data\n",
    "train_data, train_ids = extract_data('train')\n",
    "test_data, test_ids = extract_data('test')\n",
    "\n",
    "# Load true labels for test data\n",
    "test_labels = list(pd.read_csv('test_ground_truth').loc[:, 'label'])\n",
    "\n",
    "print('Train and Test data upload succesfully!')\n",
    "\n",
    "# Feature extraction using the word2simple_features function\n",
    "train_features = [sent2features(s, feature_func=word2better_features_tag) for s in train_data]\n",
    "train_labels = [sent2labels(s) for s in train_data]\n",
    "test_features = [sent2features(s, feature_func=word2better_features_tag) for s in test_data]\n",
    "\n",
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "for xseq, yseq in zip(train_features, train_labels):\n",
    "    trainer.append(xseq, yseq)\n",
    "print('Feature Extraction done!')    \n",
    "\n",
    "# Explore the extracted features    \n",
    "sent2features(train_data[0], word2simple_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the classifier parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature.minfreq',\n",
       " 'feature.possible_states',\n",
       " 'feature.possible_transitions',\n",
       " 'c1',\n",
       " 'c2',\n",
       " 'max_iterations',\n",
       " 'num_memories',\n",
       " 'epsilon',\n",
       " 'period',\n",
       " 'delta',\n",
       " 'linesearch',\n",
       " 'max_linesearch']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the classifier parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.set_params({\n",
    "    'c1': 100.0,   # coefficient for L1 penalty\n",
    "    'c2': 1e-3,  # coefficient for L2 penalty\n",
    "    'max_iterations': 50,  # stop earlier\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done :)\n",
      "CPU times: user 17 s, sys: 12 ms, total: 17 s\n",
      "Wall time: 17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train('ner-esp.model')\n",
    "\n",
    "print('Training done :)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions with your NER model\n",
    "Make predictions and evaluate your model on the test set.\n",
    "To use your NER model, create pycrfsuite.Tagger, open the model, and use the \"tag\" method, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.02      0.97      0.04        38\n",
      "     B-MISC       0.00      0.12      0.00         8\n",
      "     I-MISC       0.00      0.00      0.00      1952\n",
      "      B-ORG       0.12      0.97      0.21       388\n",
      "      I-ORG       0.00      0.03      0.00        31\n",
      "      B-PER       0.02      0.69      0.03        48\n",
      "      I-PER       0.03      0.79      0.06        67\n",
      "\n",
      "avg / total       0.02      0.20      0.04      2532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('ner-esp.model')\n",
    "test_pred = [tagger.tag(xseq) for xseq in test_features]\n",
    "test_pred = [s for w in test_pred for s in w]\n",
    "\n",
    "# Generate Kaggle file\n",
    "generate_kaggle_res_file(test_ids, test_pred, 'result.csv')\n",
    "\n",
    "## Print evaluation\n",
    "print(bio_classification_report(test_pred, test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 {'loss': 100898.779131, 'error_norm': 3205.740655, 'linesearch_trials': 1, 'active_features': 224, 'num': 50, 'time': 0.157, 'scores': {}, 'linesearch_step': 1.0, 'feature_norm': 17.142624}\n"
     ]
    }
   ],
   "source": [
    "print (len(trainer.logparser.iterations), trainer.logparser.iterations[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check what the classifier has learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top likely transitions:\n",
      "B-PER  -> I-PER   5.106086\n",
      "I-ORG  -> I-ORG   4.577166\n",
      "B-MISC -> I-MISC  4.393290\n",
      "I-MISC -> I-MISC  4.270381\n",
      "I-LOC  -> I-LOC   4.204211\n",
      "B-ORG  -> I-ORG   4.126288\n",
      "B-LOC  -> I-LOC   3.718146\n",
      "I-PER  -> I-PER   3.667023\n",
      "O      -> B-ORG   2.404751\n",
      "O      -> B-LOC   1.634268\n",
      "O      -> O       1.560973\n",
      "O      -> B-MISC  1.326638\n",
      "O      -> B-PER   1.316798\n",
      "B-ORG  -> O       0.586847\n",
      "B-LOC  -> O       0.363073\n",
      "\n",
      "Top unlikely transitions:\n",
      "O      -> O       1.560973\n",
      "O      -> B-MISC  1.326638\n",
      "O      -> B-PER   1.316798\n",
      "B-ORG  -> O       0.586847\n",
      "B-LOC  -> O       0.363073\n",
      "I-PER  -> O       0.265646\n",
      "I-ORG  -> O       0.018350\n",
      "I-ORG  -> I-MISC  -0.000135\n",
      "B-MISC -> O       -0.056457\n",
      "I-LOC  -> O       -0.123269\n",
      "I-MISC -> O       -0.306588\n",
      "O      -> I-LOC   -1.506816\n",
      "O      -> I-MISC  -2.224771\n",
      "O      -> I-PER   -2.417485\n",
      "O      -> I-ORG   -2.586834\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "info = tagger.info()\n",
    "\n",
    "def print_transitions(trans_features):\n",
    "    for (label_from, label_to), weight in trans_features:\n",
    "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "print(\"Top likely transitions:\")\n",
    "print_transitions(Counter(info.transitions).most_common(15))\n",
    "\n",
    "print(\"\\nTop unlikely transitions:\")\n",
    "print_transitions(Counter(info.transitions).most_common()[-15:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, for example, it is very likely that the beginning of a person name (B-PER) will be followed by a token inside person name (I-PER). Also note O -> B-LOC are penalized.\n",
    "\n",
    "## Check the state features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top positive:\n",
      "2.562545 O      word.lower():el\n",
      "2.328283 O      bias\n",
      "2.295047 O      EOS\n",
      "2.139253 O      word.lower():en\n",
      "2.137480 I-PER  word[-2:]:ez\n",
      "2.084413 B-ORG  word.lower():efe\n",
      "2.001973 B-ORG  word.lower():gobierno\n",
      "1.790164 B-LOC  BOS\n",
      "1.670007 O      word.lower():con\n",
      "1.605402 B-PER  BOS\n",
      "1.426642 O      word.lower():para\n",
      "1.372486 O      word.lower():una\n",
      "1.365916 O      word.lower():,\n",
      "1.365916 O      word[-2:]:,\n",
      "1.365916 O      word[-3:]:,\n",
      "1.352841 O      word[-2:]:se\n",
      "1.308297 O      word[-2:]:de\n",
      "1.269299 O      word.lower():la\n",
      "1.257194 B-ORG  word[-2:]:FE\n",
      "1.216900 B-ORG  word[-3:]:EFE\n",
      "\n",
      "Top negative:\n",
      "-0.262594 I-PER  bias\n",
      "-0.280604 O      word[-2:]:ga\n",
      "-0.286225 O      word[-3:]:uel\n",
      "-0.292385 B-ORG  word[-2:]:es\n",
      "-0.298937 O      word[-2:]:sé\n",
      "-0.325356 B-PER  word.lower():la\n",
      "-0.328440 O      word[-3:]:rra\n",
      "-0.418856 O      word[-3:]:opa\n",
      "-0.418973 I-LOC  bias\n",
      "-0.436460 B-MISC bias\n",
      "-0.445661 O      word[-2:]:pa\n",
      "-0.477377 O      word[-3:]:ina\n",
      "-0.498014 I-ORG  bias\n",
      "-0.572642 B-ORG  word[-2:]:de\n",
      "-0.594455 O      word.lower():efe\n",
      "-0.648540 O      word[-3:]:ona\n",
      "-0.655822 O      word[-2:]:ez\n",
      "-0.687738 B-ORG  bias\n",
      "-0.729016 O      word[-2:]:ia\n",
      "-0.896983 O      word[-2:]:ña\n"
     ]
    }
   ],
   "source": [
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-6s %s\" % (weight, label, attr))    \n",
    "\n",
    "print(\"Top positive:\")\n",
    "print_state_features(Counter(info.state_features).most_common(20))\n",
    "\n",
    "print(\"\\nTop negative:\")\n",
    "print_state_features(Counter(info.state_features).most_common()[-20:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
